{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "filled-luxembourg",
   "metadata": {},
   "source": [
    "# Importing libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stretch-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-growth",
   "metadata": {},
   "source": [
    "# Creating Dataset\n",
    "\n",
    "- Dataset was first convert to csv file with 2 columns Tags and Word\n",
    "- File was imported, Cleaned and Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "short-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset = dataset.dropna()\n",
    "dataset.to_csv(\"dataset.csv\", index = False)\n",
    "\n",
    "dataset = pd.read_csv(\"dataset_test.csv\")\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "dataset.to_csv(\"dataset_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-prior",
   "metadata": {},
   "source": [
    "# Creating data and labels\n",
    "\n",
    "- Dataset was divided 4 file\n",
    "1. X_train : Contains Training Words\n",
    "2. X_test : Contains Training Tags\n",
    "3. Y_train : Contains Test Words\n",
    "4. Y_test : Contains Test Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "massive-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "x_train = pd.read_csv(\"x_train.csv\")\n",
    "x_train[\"Word\"] = x_train[\"Word\"].apply(str)\n",
    "\n",
    "x_test = pd.read_csv(\"x_test.csv\")\n",
    "x_test[\"Word\"] = x_test[\"Word\"].apply(str)\n",
    "\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-college",
   "metadata": {},
   "source": [
    "# Label Encoding\n",
    "\n",
    "- Tags are labelled using label encoder and one hot encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moral-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "y_train[\"Tag\"] = labelencoder.fit_transform(y_train[\"Tag\"])\n",
    "y_test[\"Tag\"] = labelencoder.transform(y_test[\"Tag\"])\n",
    "\n",
    "onehotencoder = OneHotEncoder() \n",
    "\n",
    "Y_train = onehotencoder.fit_transform(y_train[\"Tag\"].values.reshape(-1,1)).toarray()\n",
    "Y_test = onehotencoder.transform(y_test[\"Tag\"].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-headset",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "\n",
    "- Tokenisation of words and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "african-transsexual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70525 train sequences\n",
      "14256 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (70525, 1)\n",
      "x_test shape: (14256, 1)\n"
     ]
    }
   ],
   "source": [
    "max_features = 80000\n",
    "maxlen = 1\n",
    "batch_size = 32\n",
    "\n",
    "x_train = x_train['Word'].fillna('').tolist()\n",
    "\n",
    "x_test = x_test['Word'].fillna('').tolist()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 200000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-prison",
   "metadata": {},
   "source": [
    "# Building Model\n",
    "\n",
    "- Building LSTM model using Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saving-ireland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-royalty",
   "metadata": {},
   "source": [
    "# Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "freelance-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/3\n",
      "2204/2204 [==============================] - 215s 96ms/step - loss: 1.2458 - accuracy: 0.6703 - val_loss: 0.6896 - val_accuracy: 0.8043\n",
      "Epoch 2/3\n",
      "2204/2204 [==============================] - 224s 102ms/step - loss: 0.6040 - accuracy: 0.8296 - val_loss: 0.6392 - val_accuracy: 0.8095\n",
      "Epoch 3/3\n",
      "2204/2204 [==============================] - 216s 98ms/step - loss: 0.5431 - accuracy: 0.8413 - val_loss: 0.6230 - val_accuracy: 0.8166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbcf13c52b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "proof-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-speaking",
   "metadata": {},
   "source": [
    "# Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prerequisite-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 3s 6ms/step - loss: 2.8266 - accuracy: 0.6074\n",
      "Test score: 2.8265540599823\n",
      "Test accuracy: 0.6073933839797974\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(pred, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-filling",
   "metadata": {},
   "source": [
    "# Saving model and other objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "headed-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"rnn.hdf5\")\n",
    "\n",
    "with open('labelencoder', 'wb') as f:\n",
    "    pickle.dump(labelencoder, f)\n",
    "    \n",
    "with open('onehotencoder', 'wb') as f:\n",
    "    pickle.dump(onehotencoder, f)\n",
    "    \n",
    "with open('tokenizer', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
